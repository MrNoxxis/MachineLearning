{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ **FASE IV: RF MODEL**ðŸŸ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sbn\n",
    "from seaborn import kdeplot\n",
    "from seaborn import lmplot\n",
    "from seaborn import boxplot\n",
    "\n",
    "import warnings #Advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Mostrar todas las columnas\n",
    "pd.options.display.max_columns = 70\n",
    "pd.options.display.max_rows = 70\n",
    "#Ajuste de formato para evitar notacion cientÃ­fica\n",
    "pd.options.display.float_format = '{:20,.5f}'.format\n",
    "\n",
    "# Carga de parquet preprocesado\n",
    "path = 'monopoly_cleaned_parquet'\n",
    "monopoly = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51121, 66)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monopoly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subsegmento</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>Region</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Renta</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Internauta</th>\n",
       "      <th>Adicional</th>\n",
       "      <th>Dualidad</th>\n",
       "      <th>Monoproducto</th>\n",
       "      <th>Ctacte</th>\n",
       "      <th>Consumo</th>\n",
       "      <th>Hipotecario</th>\n",
       "      <th>Debito</th>\n",
       "      <th>CambioPin</th>\n",
       "      <th>Cuentas</th>\n",
       "      <th>TC</th>\n",
       "      <th>CUPO_L1</th>\n",
       "      <th>CUPO_L2</th>\n",
       "      <th>CUPO_MX</th>\n",
       "      <th>target</th>\n",
       "      <th>FlgAct_sum</th>\n",
       "      <th>FlgActCN_sum</th>\n",
       "      <th>FlgActCI_sum</th>\n",
       "      <th>FlgActAN_sum</th>\n",
       "      <th>FlgActAI_sum</th>\n",
       "      <th>FlgActPAT_sum</th>\n",
       "      <th>FlgActCCPC_sum</th>\n",
       "      <th>FlgActCCOT_sum</th>\n",
       "      <th>FlgActCOL_sum</th>\n",
       "      <th>Fac_avg</th>\n",
       "      <th>Txs_avg</th>\n",
       "      <th>FacCN_avg</th>\n",
       "      <th>TxsCN_avg</th>\n",
       "      <th>FacCI_avg</th>\n",
       "      <th>TxsCI_avg</th>\n",
       "      <th>FacAN_avg</th>\n",
       "      <th>TxsAN_avg</th>\n",
       "      <th>FacAI_avg</th>\n",
       "      <th>TxsAI_avg</th>\n",
       "      <th>FacPAT_avg</th>\n",
       "      <th>TxsPAT_avg</th>\n",
       "      <th>FacCCPC_avg</th>\n",
       "      <th>TxsCCPC_avg</th>\n",
       "      <th>FacCCOT_avg</th>\n",
       "      <th>TxsCCOT_avg</th>\n",
       "      <th>FacCOL_avg</th>\n",
       "      <th>TxsCOL_avg</th>\n",
       "      <th>FacDebCom_avg</th>\n",
       "      <th>TxsDebCom_avg</th>\n",
       "      <th>FacDebAtm_avg</th>\n",
       "      <th>TxsDebAtm_avg</th>\n",
       "      <th>Col_avg</th>\n",
       "      <th>ColL1T0_avg</th>\n",
       "      <th>ColL1TE_avg</th>\n",
       "      <th>ColL2T0_avg</th>\n",
       "      <th>ColL2AC_avg</th>\n",
       "      <th>ColL2CC_avg</th>\n",
       "      <th>ColMx_avg</th>\n",
       "      <th>PagoNac_avg</th>\n",
       "      <th>PagoInt_avg</th>\n",
       "      <th>EeccNac_avg</th>\n",
       "      <th>EeccInt_avg</th>\n",
       "      <th>UsoL1_avg</th>\n",
       "      <th>UsoL2_avg</th>\n",
       "      <th>UsoLI_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>43.00000</td>\n",
       "      <td>601,932.80000</td>\n",
       "      <td>130.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>798,000.00000</td>\n",
       "      <td>1,012,000.00000</td>\n",
       "      <td>1,210.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>32,185.25000</td>\n",
       "      <td>1.91667</td>\n",
       "      <td>32,185.25000</td>\n",
       "      <td>1.91667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>15,511.25000</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>6,460.75000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>178,764.16667</td>\n",
       "      <td>28.91667</td>\n",
       "      <td>99,000.00000</td>\n",
       "      <td>4.25000</td>\n",
       "      <td>920,109.33333</td>\n",
       "      <td>15,059.41667</td>\n",
       "      <td>842,900.75000</td>\n",
       "      <td>27,449.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>34,699.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>29,333.33333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>908,079.41667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>913,045.58333</td>\n",
       "      <td>53,874.25000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>46.00000</td>\n",
       "      <td>143,640.00000</td>\n",
       "      <td>69.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>750,000.00000</td>\n",
       "      <td>150,000.00000</td>\n",
       "      <td>1,000.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>112,918.44812</td>\n",
       "      <td>3.66667</td>\n",
       "      <td>81,006.83333</td>\n",
       "      <td>2.58333</td>\n",
       "      <td>31,911.61478</td>\n",
       "      <td>1.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>62,203.66667</td>\n",
       "      <td>1.58333</td>\n",
       "      <td>8,916.66667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>513,504.12917</td>\n",
       "      <td>7,469.83333</td>\n",
       "      <td>309,153.41667</td>\n",
       "      <td>133,584.41667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17,597.25000</td>\n",
       "      <td>45,699.21250</td>\n",
       "      <td>167,416.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>460,136.50000</td>\n",
       "      <td>58.90417</td>\n",
       "      <td>428,734.83333</td>\n",
       "      <td>119,963.91667</td>\n",
       "      <td>84.50833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>929,106.00000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1,350,000.00000</td>\n",
       "      <td>200,000.00000</td>\n",
       "      <td>1,500.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>148,982.41667</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>140,434.33333</td>\n",
       "      <td>4.66667</td>\n",
       "      <td>8,548.08333</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>118,568.66667</td>\n",
       "      <td>6.75000</td>\n",
       "      <td>741,071.75000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>117,984.83333</td>\n",
       "      <td>117,984.83333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>68,325.75000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8,510.75000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>25,294.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>46.00000</td>\n",
       "      <td>172,447.00000</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1,570,800.00000</td>\n",
       "      <td>220,001.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>34,523.83333</td>\n",
       "      <td>2.08333</td>\n",
       "      <td>34,523.83333</td>\n",
       "      <td>2.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>20,909.16667</td>\n",
       "      <td>1.16667</td>\n",
       "      <td>1,082.50000</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>340,728.08333</td>\n",
       "      <td>14,826.16667</td>\n",
       "      <td>295,497.25000</td>\n",
       "      <td>27,698.16667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2,706.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>57,083.33333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>341,916.83333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>344,417.50000</td>\n",
       "      <td>31,825.25000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>46.00000</td>\n",
       "      <td>805,250.00000</td>\n",
       "      <td>116.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2,762,000.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>6,430.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>293,939.00000</td>\n",
       "      <td>8.91667</td>\n",
       "      <td>293,939.00000</td>\n",
       "      <td>8.91667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>228,901.25000</td>\n",
       "      <td>4.33333</td>\n",
       "      <td>3,916.66667</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>242,907.25000</td>\n",
       "      <td>51,661.25000</td>\n",
       "      <td>17.66667</td>\n",
       "      <td>185,000.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6,227.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>291,317.16667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>291,684.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>83,849.25000</td>\n",
       "      <td>402,597.66667</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>170.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>47.00000</td>\n",
       "      <td>707,664.00000</td>\n",
       "      <td>67.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1,312,500.00000</td>\n",
       "      <td>450,000.00000</td>\n",
       "      <td>714.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>60,771.16667</td>\n",
       "      <td>2.41667</td>\n",
       "      <td>60,771.16667</td>\n",
       "      <td>2.41667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30,270.25000</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>9,729.16667</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>22,588.50000</td>\n",
       "      <td>2.16667</td>\n",
       "      <td>247,333.33333</td>\n",
       "      <td>8.50000</td>\n",
       "      <td>80,259.33333</td>\n",
       "      <td>2,274.00000</td>\n",
       "      <td>975.83333</td>\n",
       "      <td>46,191.91667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30,817.58333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>67,995.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>66,314.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>761.50000</td>\n",
       "      <td>68,509.25000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>811.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>48.00000</td>\n",
       "      <td>1,022,833.00000</td>\n",
       "      <td>21.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>813,000.00000</td>\n",
       "      <td>160,000.00000</td>\n",
       "      <td>600.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>150,786.66667</td>\n",
       "      <td>12.50000</td>\n",
       "      <td>142,519.08333</td>\n",
       "      <td>11.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8,267.58333</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>42,460.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2,481.58333</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>149,398.33333</td>\n",
       "      <td>9.50000</td>\n",
       "      <td>183,666.66667</td>\n",
       "      <td>5.83333</td>\n",
       "      <td>396,062.41667</td>\n",
       "      <td>118,888.08333</td>\n",
       "      <td>190,794.00000</td>\n",
       "      <td>57,066.83333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>29,313.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>229,371.41667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>360,648.41667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>384,923.58333</td>\n",
       "      <td>72,453.33333</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>170.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>46.00000</td>\n",
       "      <td>544,956.40000</td>\n",
       "      <td>69.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1,242,000.00000</td>\n",
       "      <td>1,738,000.00000</td>\n",
       "      <td>1,255.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>39,667.66667</td>\n",
       "      <td>1.91667</td>\n",
       "      <td>39,667.66667</td>\n",
       "      <td>1.91667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>14,644.00000</td>\n",
       "      <td>0.41667</td>\n",
       "      <td>2,499.16667</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>306,833.33333</td>\n",
       "      <td>13.66667</td>\n",
       "      <td>115,805.66667</td>\n",
       "      <td>25,040.33333</td>\n",
       "      <td>61,641.58333</td>\n",
       "      <td>24,053.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5,070.75000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>21,030.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>104,602.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>108,996.83333</td>\n",
       "      <td>30,391.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>170.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>49.00000</td>\n",
       "      <td>1,171,066.00000</td>\n",
       "      <td>33.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1,996,400.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2,000.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>27,503.55413</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>21,788.25000</td>\n",
       "      <td>0.41667</td>\n",
       "      <td>5,715.30413</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>15,228.41667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>678,200.16667</td>\n",
       "      <td>9.83333</td>\n",
       "      <td>481,029.72083</td>\n",
       "      <td>6,559.83333</td>\n",
       "      <td>438,741.16667</td>\n",
       "      <td>15,228.41667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>14,785.00000</td>\n",
       "      <td>5,715.30417</td>\n",
       "      <td>46,666.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>479,423.41667</td>\n",
       "      <td>10.84333</td>\n",
       "      <td>479,423.41667</td>\n",
       "      <td>24,828.66667</td>\n",
       "      <td>10.84333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>170.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>44.00000</td>\n",
       "      <td>964,387.00000</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>672,000.00000</td>\n",
       "      <td>3,500,000.00000</td>\n",
       "      <td>1,000.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>55,655.58333</td>\n",
       "      <td>2.16667</td>\n",
       "      <td>47,572.25000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8,083.33333</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11,180.41667</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>42,919.08333</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>55,916.66667</td>\n",
       "      <td>1.25000</td>\n",
       "      <td>59,458.91667</td>\n",
       "      <td>36,222.58333</td>\n",
       "      <td>8,132.50000</td>\n",
       "      <td>15,103.83333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>354,215.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>55,744.75000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>50,686.91667</td>\n",
       "      <td>16,111.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Subsegmento                 Sexo               Region  \\\n",
       "0            160.00000              1.00000             13.00000   \n",
       "1            160.00000              0.00000             13.00000   \n",
       "2            170.00000              0.00000             13.00000   \n",
       "3            151.00000              0.00000             13.00000   \n",
       "4            170.00000              0.00000             13.00000   \n",
       "5            170.00000              0.00000             13.00000   \n",
       "6            811.00000              0.00000             13.00000   \n",
       "7            170.00000              0.00000             13.00000   \n",
       "8            170.00000              0.00000             13.00000   \n",
       "9            170.00000              1.00000             13.00000   \n",
       "\n",
       "                  Edad                Renta           Antiguedad  \\\n",
       "0             43.00000        601,932.80000            130.00000   \n",
       "1             46.00000        143,640.00000             69.00000   \n",
       "2             45.00000        929,106.00000             24.00000   \n",
       "3             46.00000        172,447.00000            134.00000   \n",
       "4             46.00000        805,250.00000            116.00000   \n",
       "5             47.00000        707,664.00000             67.00000   \n",
       "6             48.00000      1,022,833.00000             21.00000   \n",
       "7             46.00000        544,956.40000             69.00000   \n",
       "8             49.00000      1,171,066.00000             33.00000   \n",
       "9             44.00000        964,387.00000             23.00000   \n",
       "\n",
       "            Internauta            Adicional             Dualidad  \\\n",
       "0              1.00000              1.00000              0.00000   \n",
       "1              1.00000              0.00000              0.00000   \n",
       "2              1.00000              1.00000              0.00000   \n",
       "3              0.00000              1.00000              0.00000   \n",
       "4              0.00000              1.00000              1.00000   \n",
       "5              1.00000              1.00000              0.00000   \n",
       "6              1.00000              0.00000              1.00000   \n",
       "7              0.00000              1.00000              1.00000   \n",
       "8              0.00000              0.00000              0.00000   \n",
       "9              1.00000              1.00000              0.00000   \n",
       "\n",
       "          Monoproducto               Ctacte              Consumo  \\\n",
       "0              0.00000              1.00000              0.00000   \n",
       "1              0.00000              1.00000              0.00000   \n",
       "2              0.00000              1.00000              0.00000   \n",
       "3              1.00000              0.00000              0.00000   \n",
       "4              0.00000              1.00000              0.00000   \n",
       "5              0.00000              1.00000              0.00000   \n",
       "6              0.00000              1.00000              0.00000   \n",
       "7              0.00000              1.00000              0.00000   \n",
       "8              0.00000              1.00000              0.00000   \n",
       "9              0.00000              1.00000              0.00000   \n",
       "\n",
       "           Hipotecario               Debito            CambioPin  \\\n",
       "0              0.00000              1.00000              0.00000   \n",
       "1              1.00000              0.00000              0.00000   \n",
       "2              1.00000              1.00000              1.00000   \n",
       "3              0.00000              0.00000              1.00000   \n",
       "4              1.00000              0.00000              1.00000   \n",
       "5              0.00000              1.00000              1.00000   \n",
       "6              0.00000              1.00000              1.00000   \n",
       "7              0.00000              1.00000              0.00000   \n",
       "8              0.00000              1.00000              1.00000   \n",
       "9              1.00000              1.00000              1.00000   \n",
       "\n",
       "               Cuentas                   TC              CUPO_L1  \\\n",
       "0              1.00000              3.00000        798,000.00000   \n",
       "1              1.00000              1.00000        750,000.00000   \n",
       "2              1.00000              2.00000      1,350,000.00000   \n",
       "3              1.00000              2.00000      1,570,800.00000   \n",
       "4              2.00000              3.00000      2,762,000.00000   \n",
       "5              1.00000              2.00000      1,312,500.00000   \n",
       "6              2.00000              2.00000        813,000.00000   \n",
       "7              2.00000              3.00000      1,242,000.00000   \n",
       "8              1.00000              1.00000      1,996,400.00000   \n",
       "9              1.00000              3.00000        672,000.00000   \n",
       "\n",
       "               CUPO_L2              CUPO_MX               target  \\\n",
       "0      1,012,000.00000          1,210.00000              0.00000   \n",
       "1        150,000.00000          1,000.00000              0.00000   \n",
       "2        200,000.00000          1,500.00000              0.00000   \n",
       "3        220,001.00000              0.00000              0.00000   \n",
       "4              2.00000          6,430.00000              0.00000   \n",
       "5        450,000.00000            714.00000              0.00000   \n",
       "6        160,000.00000            600.00000              0.00000   \n",
       "7      1,738,000.00000          1,255.00000              0.00000   \n",
       "8              1.00000          2,000.00000              1.00000   \n",
       "9      3,500,000.00000          1,000.00000              0.00000   \n",
       "\n",
       "            FlgAct_sum         FlgActCN_sum         FlgActCI_sum  \\\n",
       "0              6.00000              6.00000              0.00000   \n",
       "1             11.00000             10.00000              2.00000   \n",
       "2             12.00000             12.00000              1.00000   \n",
       "3             11.00000             11.00000              0.00000   \n",
       "4             12.00000             12.00000              0.00000   \n",
       "5              9.00000              9.00000              0.00000   \n",
       "6             12.00000             10.00000              0.00000   \n",
       "7              9.00000              9.00000              0.00000   \n",
       "8              6.00000              5.00000              1.00000   \n",
       "9              9.00000              9.00000              0.00000   \n",
       "\n",
       "          FlgActAN_sum         FlgActAI_sum        FlgActPAT_sum  \\\n",
       "0              0.00000              0.00000              0.00000   \n",
       "1              0.00000              0.00000              0.00000   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3              0.00000              0.00000              0.00000   \n",
       "4              0.00000              0.00000              0.00000   \n",
       "5              0.00000              0.00000              0.00000   \n",
       "6              0.00000              0.00000             12.00000   \n",
       "7              0.00000              0.00000              0.00000   \n",
       "8              0.00000              0.00000              0.00000   \n",
       "9              1.00000              0.00000              0.00000   \n",
       "\n",
       "        FlgActCCPC_sum       FlgActCCOT_sum        FlgActCOL_sum  \\\n",
       "0              4.00000              1.00000              0.00000   \n",
       "1              9.00000              3.00000              0.00000   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3              9.00000              1.00000              0.00000   \n",
       "4             12.00000              1.00000              0.00000   \n",
       "5              6.00000              3.00000              0.00000   \n",
       "6              7.00000              1.00000              0.00000   \n",
       "7              4.00000              1.00000              0.00000   \n",
       "8              4.00000              0.00000              0.00000   \n",
       "9              2.00000              0.00000              0.00000   \n",
       "\n",
       "               Fac_avg              Txs_avg            FacCN_avg  \\\n",
       "0         32,185.25000              1.91667         32,185.25000   \n",
       "1        112,918.44812              3.66667         81,006.83333   \n",
       "2        148,982.41667              5.00000        140,434.33333   \n",
       "3         34,523.83333              2.08333         34,523.83333   \n",
       "4        293,939.00000              8.91667        293,939.00000   \n",
       "5         60,771.16667              2.41667         60,771.16667   \n",
       "6        150,786.66667             12.50000        142,519.08333   \n",
       "7         39,667.66667              1.91667         39,667.66667   \n",
       "8         27,503.55413              0.50000         21,788.25000   \n",
       "9         55,655.58333              2.16667         47,572.25000   \n",
       "\n",
       "             TxsCN_avg            FacCI_avg            TxsCI_avg  \\\n",
       "0              1.91667              0.00000              0.00000   \n",
       "1              2.58333         31,911.61478              1.08333   \n",
       "2              4.66667          8,548.08333              0.33333   \n",
       "3              2.08333              0.00000              0.00000   \n",
       "4              8.91667              0.00000              0.00000   \n",
       "5              2.41667              0.00000              0.00000   \n",
       "6             11.50000              0.00000              0.00000   \n",
       "7              1.91667              0.00000              0.00000   \n",
       "8              0.41667          5,715.30413              0.08333   \n",
       "9              2.00000              0.00000              0.00000   \n",
       "\n",
       "             FacAN_avg            TxsAN_avg            FacAI_avg  \\\n",
       "0              0.00000              0.00000              0.00000   \n",
       "1              0.00000              0.00000              0.00000   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3              0.00000              0.00000              0.00000   \n",
       "4              0.00000              0.00000              0.00000   \n",
       "5              0.00000              0.00000              0.00000   \n",
       "6              0.00000              0.00000              0.00000   \n",
       "7              0.00000              0.00000              0.00000   \n",
       "8              0.00000              0.00000              0.00000   \n",
       "9          8,083.33333              0.16667              0.00000   \n",
       "\n",
       "             TxsAI_avg           FacPAT_avg           TxsPAT_avg  \\\n",
       "0              0.00000              0.00000              0.00000   \n",
       "1              0.00000              0.00000              0.00000   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3              0.00000              0.00000              0.00000   \n",
       "4              0.00000              0.00000              0.00000   \n",
       "5              0.00000              0.00000              0.00000   \n",
       "6              0.00000          8,267.58333              1.00000   \n",
       "7              0.00000              0.00000              0.00000   \n",
       "8              0.00000              0.00000              0.00000   \n",
       "9              0.00000              0.00000              0.00000   \n",
       "\n",
       "           FacCCPC_avg          TxsCCPC_avg          FacCCOT_avg  \\\n",
       "0         15,511.25000              0.66667          6,460.75000   \n",
       "1         62,203.66667              1.58333          8,916.66667   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3         20,909.16667              1.16667          1,082.50000   \n",
       "4        228,901.25000              4.33333          3,916.66667   \n",
       "5         30,270.25000              0.83333          9,729.16667   \n",
       "6         42,460.00000              1.00000          2,481.58333   \n",
       "7         14,644.00000              0.41667          2,499.16667   \n",
       "8         15,228.41667              0.33333              0.00000   \n",
       "9         11,180.41667              0.25000              0.00000   \n",
       "\n",
       "           TxsCCOT_avg           FacCOL_avg           TxsCOL_avg  \\\n",
       "0              0.25000              0.00000              0.00000   \n",
       "1              0.33333              0.00000              0.00000   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3              0.08333              0.00000              0.00000   \n",
       "4              0.08333              0.00000              0.00000   \n",
       "5              0.50000              0.00000              0.00000   \n",
       "6              0.08333              0.00000              0.00000   \n",
       "7              0.08333              0.00000              0.00000   \n",
       "8              0.00000              0.00000              0.00000   \n",
       "9              0.00000              0.00000              0.00000   \n",
       "\n",
       "         FacDebCom_avg        TxsDebCom_avg        FacDebAtm_avg  \\\n",
       "0        178,764.16667             28.91667         99,000.00000   \n",
       "1              0.00000              0.00000              0.00000   \n",
       "2        118,568.66667              6.75000        741,071.75000   \n",
       "3              0.00000              0.00000              0.00000   \n",
       "4              0.00000              0.00000              0.00000   \n",
       "5         22,588.50000              2.16667        247,333.33333   \n",
       "6        149,398.33333              9.50000        183,666.66667   \n",
       "7              0.00000              0.00000        306,833.33333   \n",
       "8              0.00000              0.00000        678,200.16667   \n",
       "9         42,919.08333              2.00000         55,916.66667   \n",
       "\n",
       "         TxsDebAtm_avg              Col_avg          ColL1T0_avg  \\\n",
       "0              4.25000        920,109.33333         15,059.41667   \n",
       "1              0.00000        513,504.12917          7,469.83333   \n",
       "2             15.00000        117,984.83333        117,984.83333   \n",
       "3              0.00000        340,728.08333         14,826.16667   \n",
       "4              0.00000        242,907.25000         51,661.25000   \n",
       "5              8.50000         80,259.33333          2,274.00000   \n",
       "6              5.83333        396,062.41667        118,888.08333   \n",
       "7             13.66667        115,805.66667         25,040.33333   \n",
       "8              9.83333        481,029.72083          6,559.83333   \n",
       "9              1.25000         59,458.91667         36,222.58333   \n",
       "\n",
       "           ColL1TE_avg          ColL2T0_avg          ColL2AC_avg  \\\n",
       "0        842,900.75000         27,449.50000              0.00000   \n",
       "1        309,153.41667        133,584.41667              0.00000   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3        295,497.25000         27,698.16667              0.00000   \n",
       "4             17.66667        185,000.66667              0.00000   \n",
       "5            975.83333         46,191.91667              0.00000   \n",
       "6        190,794.00000         57,066.83333              0.00000   \n",
       "7         61,641.58333         24,053.00000              0.00000   \n",
       "8        438,741.16667         15,228.41667              0.00000   \n",
       "9          8,132.50000         15,103.83333              0.00000   \n",
       "\n",
       "           ColL2CC_avg            ColMx_avg          PagoNac_avg  \\\n",
       "0         34,699.66667              0.00000         29,333.33333   \n",
       "1         17,597.25000         45,699.21250        167,416.66667   \n",
       "2              0.00000              0.00000         68,325.75000   \n",
       "3          2,706.50000              0.00000         57,083.33333   \n",
       "4          6,227.66667              0.00000        291,317.16667   \n",
       "5         30,817.58333              0.00000         67,995.50000   \n",
       "6         29,313.50000              0.00000        229,371.41667   \n",
       "7          5,070.75000              0.00000         21,030.50000   \n",
       "8         14,785.00000          5,715.30417         46,666.66667   \n",
       "9              0.00000              0.00000        354,215.08333   \n",
       "\n",
       "           PagoInt_avg          EeccNac_avg          EeccInt_avg  \\\n",
       "0              0.00000        908,079.41667              0.00000   \n",
       "1              0.00000        460,136.50000             58.90417   \n",
       "2              0.00000          8,510.75000              0.00000   \n",
       "3              0.00000        341,916.83333              0.00000   \n",
       "4              0.00000        291,684.50000              0.00000   \n",
       "5              0.00000         66,314.08333              0.00000   \n",
       "6              0.00000        360,648.41667              0.00000   \n",
       "7              0.00000        104,602.66667              0.00000   \n",
       "8              0.00000        479,423.41667             10.84333   \n",
       "9              0.00000         55,744.75000              0.00000   \n",
       "\n",
       "             UsoL1_avg            UsoL2_avg            UsoLI_avg  \n",
       "0        913,045.58333         53,874.25000              0.00000  \n",
       "1        428,734.83333        119,963.91667             84.50833  \n",
       "2         25,294.50000              0.00000              0.00000  \n",
       "3        344,417.50000         31,825.25000              0.00000  \n",
       "4         83,849.25000        402,597.66667              0.00000  \n",
       "5            761.50000         68,509.25000              0.00000  \n",
       "6        384,923.58333         72,453.33333              0.00000  \n",
       "7        108,996.83333         30,391.00000              0.00000  \n",
       "8        479,423.41667         24,828.66667             10.84333  \n",
       "9         50,686.91667         16,111.00000              0.00000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monopoly.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51121 entries, 0 to 51120\n",
      "Data columns (total 66 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Subsegmento     51121 non-null  float64\n",
      " 1   Sexo            51121 non-null  float64\n",
      " 2   Region          51121 non-null  float64\n",
      " 3   Edad            51121 non-null  float64\n",
      " 4   Renta           51121 non-null  float64\n",
      " 5   Antiguedad      51121 non-null  float64\n",
      " 6   Internauta      51121 non-null  float64\n",
      " 7   Adicional       51121 non-null  float64\n",
      " 8   Dualidad        51121 non-null  float64\n",
      " 9   Monoproducto    51121 non-null  float64\n",
      " 10  Ctacte          51121 non-null  float64\n",
      " 11  Consumo         51121 non-null  float64\n",
      " 12  Hipotecario     51121 non-null  float64\n",
      " 13  Debito          51121 non-null  float64\n",
      " 14  CambioPin       51121 non-null  float64\n",
      " 15  Cuentas         51121 non-null  float64\n",
      " 16  TC              51121 non-null  float64\n",
      " 17  CUPO_L1         51121 non-null  float64\n",
      " 18  CUPO_L2         51121 non-null  float64\n",
      " 19  CUPO_MX         51121 non-null  float64\n",
      " 20  target          51121 non-null  float64\n",
      " 21  FlgAct_sum      51121 non-null  float64\n",
      " 22  FlgActCN_sum    51121 non-null  float64\n",
      " 23  FlgActCI_sum    51121 non-null  float64\n",
      " 24  FlgActAN_sum    51121 non-null  float64\n",
      " 25  FlgActAI_sum    51121 non-null  float64\n",
      " 26  FlgActPAT_sum   51121 non-null  float64\n",
      " 27  FlgActCCPC_sum  51121 non-null  float64\n",
      " 28  FlgActCCOT_sum  51121 non-null  float64\n",
      " 29  FlgActCOL_sum   51121 non-null  float64\n",
      " 30  Fac_avg         51121 non-null  float64\n",
      " 31  Txs_avg         51121 non-null  float64\n",
      " 32  FacCN_avg       51121 non-null  float64\n",
      " 33  TxsCN_avg       51121 non-null  float64\n",
      " 34  FacCI_avg       51121 non-null  float64\n",
      " 35  TxsCI_avg       51121 non-null  float64\n",
      " 36  FacAN_avg       51121 non-null  float64\n",
      " 37  TxsAN_avg       51121 non-null  float64\n",
      " 38  FacAI_avg       51121 non-null  float64\n",
      " 39  TxsAI_avg       51121 non-null  float64\n",
      " 40  FacPAT_avg      51121 non-null  float64\n",
      " 41  TxsPAT_avg      51121 non-null  float64\n",
      " 42  FacCCPC_avg     51121 non-null  float64\n",
      " 43  TxsCCPC_avg     51121 non-null  float64\n",
      " 44  FacCCOT_avg     51121 non-null  float64\n",
      " 45  TxsCCOT_avg     51121 non-null  float64\n",
      " 46  FacCOL_avg      51121 non-null  float64\n",
      " 47  TxsCOL_avg      51121 non-null  float64\n",
      " 48  FacDebCom_avg   51121 non-null  float64\n",
      " 49  TxsDebCom_avg   51121 non-null  float64\n",
      " 50  FacDebAtm_avg   51121 non-null  float64\n",
      " 51  TxsDebAtm_avg   51121 non-null  float64\n",
      " 52  Col_avg         51121 non-null  float64\n",
      " 53  ColL1T0_avg     51121 non-null  float64\n",
      " 54  ColL1TE_avg     51121 non-null  float64\n",
      " 55  ColL2T0_avg     51121 non-null  float64\n",
      " 56  ColL2AC_avg     51121 non-null  float64\n",
      " 57  ColL2CC_avg     51121 non-null  float64\n",
      " 58  ColMx_avg       51121 non-null  float64\n",
      " 59  PagoNac_avg     51121 non-null  float64\n",
      " 60  PagoInt_avg     51121 non-null  float64\n",
      " 61  EeccNac_avg     51121 non-null  float64\n",
      " 62  EeccInt_avg     51121 non-null  float64\n",
      " 63  UsoL1_avg       51121 non-null  float64\n",
      " 64  UsoL2_avg       51121 non-null  float64\n",
      " 65  UsoLI_avg       51121 non-null  float64\n",
      "dtypes: float64(66)\n",
      "memory usage: 25.7 MB\n"
     ]
    }
   ],
   "source": [
    "monopoly.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ„MODELO RANDOM FORESTðŸŽ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¶REG--MODELS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âž¡ï¸MODEL 1 ðŸŽ„-REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ - `DefiniciÃ³n de variables:` Utilizamos las variables $X$ e $Y$ previamente definidas y divididas en conjunto de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          601,932.80000\n",
       "1          143,640.00000\n",
       "2          929,106.00000\n",
       "3          172,447.00000\n",
       "4          805,250.00000\n",
       "Name: Renta, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = monopoly['Renta']\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subsegmento</th>\n",
       "      <th>Sexo</th>\n",
       "      <th>Region</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Antiguedad</th>\n",
       "      <th>Internauta</th>\n",
       "      <th>Adicional</th>\n",
       "      <th>Dualidad</th>\n",
       "      <th>Monoproducto</th>\n",
       "      <th>Ctacte</th>\n",
       "      <th>Consumo</th>\n",
       "      <th>Hipotecario</th>\n",
       "      <th>Debito</th>\n",
       "      <th>CambioPin</th>\n",
       "      <th>Cuentas</th>\n",
       "      <th>TC</th>\n",
       "      <th>CUPO_L1</th>\n",
       "      <th>CUPO_L2</th>\n",
       "      <th>CUPO_MX</th>\n",
       "      <th>target</th>\n",
       "      <th>FlgAct_sum</th>\n",
       "      <th>FlgActCN_sum</th>\n",
       "      <th>FlgActCI_sum</th>\n",
       "      <th>FlgActAN_sum</th>\n",
       "      <th>FlgActAI_sum</th>\n",
       "      <th>FlgActPAT_sum</th>\n",
       "      <th>FlgActCCPC_sum</th>\n",
       "      <th>FlgActCCOT_sum</th>\n",
       "      <th>FlgActCOL_sum</th>\n",
       "      <th>Fac_avg</th>\n",
       "      <th>Txs_avg</th>\n",
       "      <th>FacCN_avg</th>\n",
       "      <th>TxsCN_avg</th>\n",
       "      <th>FacCI_avg</th>\n",
       "      <th>TxsCI_avg</th>\n",
       "      <th>FacAN_avg</th>\n",
       "      <th>TxsAN_avg</th>\n",
       "      <th>FacAI_avg</th>\n",
       "      <th>TxsAI_avg</th>\n",
       "      <th>FacPAT_avg</th>\n",
       "      <th>TxsPAT_avg</th>\n",
       "      <th>FacCCPC_avg</th>\n",
       "      <th>TxsCCPC_avg</th>\n",
       "      <th>FacCCOT_avg</th>\n",
       "      <th>TxsCCOT_avg</th>\n",
       "      <th>FacCOL_avg</th>\n",
       "      <th>TxsCOL_avg</th>\n",
       "      <th>FacDebCom_avg</th>\n",
       "      <th>TxsDebCom_avg</th>\n",
       "      <th>FacDebAtm_avg</th>\n",
       "      <th>TxsDebAtm_avg</th>\n",
       "      <th>Col_avg</th>\n",
       "      <th>ColL1T0_avg</th>\n",
       "      <th>ColL1TE_avg</th>\n",
       "      <th>ColL2T0_avg</th>\n",
       "      <th>ColL2AC_avg</th>\n",
       "      <th>ColL2CC_avg</th>\n",
       "      <th>ColMx_avg</th>\n",
       "      <th>PagoNac_avg</th>\n",
       "      <th>PagoInt_avg</th>\n",
       "      <th>EeccNac_avg</th>\n",
       "      <th>EeccInt_avg</th>\n",
       "      <th>UsoL1_avg</th>\n",
       "      <th>UsoL2_avg</th>\n",
       "      <th>UsoLI_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>43.00000</td>\n",
       "      <td>130.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>798,000.00000</td>\n",
       "      <td>1,012,000.00000</td>\n",
       "      <td>1,210.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>32,185.25000</td>\n",
       "      <td>1.91667</td>\n",
       "      <td>32,185.25000</td>\n",
       "      <td>1.91667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>15,511.25000</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>6,460.75000</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>178,764.16667</td>\n",
       "      <td>28.91667</td>\n",
       "      <td>99,000.00000</td>\n",
       "      <td>4.25000</td>\n",
       "      <td>920,109.33333</td>\n",
       "      <td>15,059.41667</td>\n",
       "      <td>842,900.75000</td>\n",
       "      <td>27,449.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>34,699.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>29,333.33333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>908,079.41667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>913,045.58333</td>\n",
       "      <td>53,874.25000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>46.00000</td>\n",
       "      <td>69.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>750,000.00000</td>\n",
       "      <td>150,000.00000</td>\n",
       "      <td>1,000.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>112,918.44812</td>\n",
       "      <td>3.66667</td>\n",
       "      <td>81,006.83333</td>\n",
       "      <td>2.58333</td>\n",
       "      <td>31,911.61478</td>\n",
       "      <td>1.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>62,203.66667</td>\n",
       "      <td>1.58333</td>\n",
       "      <td>8,916.66667</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>513,504.12917</td>\n",
       "      <td>7,469.83333</td>\n",
       "      <td>309,153.41667</td>\n",
       "      <td>133,584.41667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17,597.25000</td>\n",
       "      <td>45,699.21250</td>\n",
       "      <td>167,416.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>460,136.50000</td>\n",
       "      <td>58.90417</td>\n",
       "      <td>428,734.83333</td>\n",
       "      <td>119,963.91667</td>\n",
       "      <td>84.50833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1,350,000.00000</td>\n",
       "      <td>200,000.00000</td>\n",
       "      <td>1,500.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>148,982.41667</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>140,434.33333</td>\n",
       "      <td>4.66667</td>\n",
       "      <td>8,548.08333</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>118,568.66667</td>\n",
       "      <td>6.75000</td>\n",
       "      <td>741,071.75000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>117,984.83333</td>\n",
       "      <td>117,984.83333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>68,325.75000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8,510.75000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>25,294.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>46.00000</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1,570,800.00000</td>\n",
       "      <td>220,001.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>34,523.83333</td>\n",
       "      <td>2.08333</td>\n",
       "      <td>34,523.83333</td>\n",
       "      <td>2.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>20,909.16667</td>\n",
       "      <td>1.16667</td>\n",
       "      <td>1,082.50000</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>340,728.08333</td>\n",
       "      <td>14,826.16667</td>\n",
       "      <td>295,497.25000</td>\n",
       "      <td>27,698.16667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2,706.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>57,083.33333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>341,916.83333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>344,417.50000</td>\n",
       "      <td>31,825.25000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>46.00000</td>\n",
       "      <td>116.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2,762,000.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>6,430.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>293,939.00000</td>\n",
       "      <td>8.91667</td>\n",
       "      <td>293,939.00000</td>\n",
       "      <td>8.91667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>228,901.25000</td>\n",
       "      <td>4.33333</td>\n",
       "      <td>3,916.66667</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>242,907.25000</td>\n",
       "      <td>51,661.25000</td>\n",
       "      <td>17.66667</td>\n",
       "      <td>185,000.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6,227.66667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>291,317.16667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>291,684.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>83,849.25000</td>\n",
       "      <td>402,597.66667</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Subsegmento                 Sexo               Region  \\\n",
       "0            160.00000              1.00000             13.00000   \n",
       "1            160.00000              0.00000             13.00000   \n",
       "2            170.00000              0.00000             13.00000   \n",
       "3            151.00000              0.00000             13.00000   \n",
       "4            170.00000              0.00000             13.00000   \n",
       "\n",
       "                  Edad           Antiguedad           Internauta  \\\n",
       "0             43.00000            130.00000              1.00000   \n",
       "1             46.00000             69.00000              1.00000   \n",
       "2             45.00000             24.00000              1.00000   \n",
       "3             46.00000            134.00000              0.00000   \n",
       "4             46.00000            116.00000              0.00000   \n",
       "\n",
       "             Adicional             Dualidad         Monoproducto  \\\n",
       "0              1.00000              0.00000              0.00000   \n",
       "1              0.00000              0.00000              0.00000   \n",
       "2              1.00000              0.00000              0.00000   \n",
       "3              1.00000              0.00000              1.00000   \n",
       "4              1.00000              1.00000              0.00000   \n",
       "\n",
       "                Ctacte              Consumo          Hipotecario  \\\n",
       "0              1.00000              0.00000              0.00000   \n",
       "1              1.00000              0.00000              1.00000   \n",
       "2              1.00000              0.00000              1.00000   \n",
       "3              0.00000              0.00000              0.00000   \n",
       "4              1.00000              0.00000              1.00000   \n",
       "\n",
       "                Debito            CambioPin              Cuentas  \\\n",
       "0              1.00000              0.00000              1.00000   \n",
       "1              0.00000              0.00000              1.00000   \n",
       "2              1.00000              1.00000              1.00000   \n",
       "3              0.00000              1.00000              1.00000   \n",
       "4              0.00000              1.00000              2.00000   \n",
       "\n",
       "                    TC              CUPO_L1              CUPO_L2  \\\n",
       "0              3.00000        798,000.00000      1,012,000.00000   \n",
       "1              1.00000        750,000.00000        150,000.00000   \n",
       "2              2.00000      1,350,000.00000        200,000.00000   \n",
       "3              2.00000      1,570,800.00000        220,001.00000   \n",
       "4              3.00000      2,762,000.00000              2.00000   \n",
       "\n",
       "               CUPO_MX               target           FlgAct_sum  \\\n",
       "0          1,210.00000              0.00000              6.00000   \n",
       "1          1,000.00000              0.00000             11.00000   \n",
       "2          1,500.00000              0.00000             12.00000   \n",
       "3              0.00000              0.00000             11.00000   \n",
       "4          6,430.00000              0.00000             12.00000   \n",
       "\n",
       "          FlgActCN_sum         FlgActCI_sum         FlgActAN_sum  \\\n",
       "0              6.00000              0.00000              0.00000   \n",
       "1             10.00000              2.00000              0.00000   \n",
       "2             12.00000              1.00000              0.00000   \n",
       "3             11.00000              0.00000              0.00000   \n",
       "4             12.00000              0.00000              0.00000   \n",
       "\n",
       "          FlgActAI_sum        FlgActPAT_sum       FlgActCCPC_sum  \\\n",
       "0              0.00000              0.00000              4.00000   \n",
       "1              0.00000              0.00000              9.00000   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3              0.00000              0.00000              9.00000   \n",
       "4              0.00000              0.00000             12.00000   \n",
       "\n",
       "        FlgActCCOT_sum        FlgActCOL_sum              Fac_avg  \\\n",
       "0              1.00000              0.00000         32,185.25000   \n",
       "1              3.00000              0.00000        112,918.44812   \n",
       "2              0.00000              0.00000        148,982.41667   \n",
       "3              1.00000              0.00000         34,523.83333   \n",
       "4              1.00000              0.00000        293,939.00000   \n",
       "\n",
       "               Txs_avg            FacCN_avg            TxsCN_avg  \\\n",
       "0              1.91667         32,185.25000              1.91667   \n",
       "1              3.66667         81,006.83333              2.58333   \n",
       "2              5.00000        140,434.33333              4.66667   \n",
       "3              2.08333         34,523.83333              2.08333   \n",
       "4              8.91667        293,939.00000              8.91667   \n",
       "\n",
       "             FacCI_avg            TxsCI_avg            FacAN_avg  \\\n",
       "0              0.00000              0.00000              0.00000   \n",
       "1         31,911.61478              1.08333              0.00000   \n",
       "2          8,548.08333              0.33333              0.00000   \n",
       "3              0.00000              0.00000              0.00000   \n",
       "4              0.00000              0.00000              0.00000   \n",
       "\n",
       "             TxsAN_avg            FacAI_avg            TxsAI_avg  \\\n",
       "0              0.00000              0.00000              0.00000   \n",
       "1              0.00000              0.00000              0.00000   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3              0.00000              0.00000              0.00000   \n",
       "4              0.00000              0.00000              0.00000   \n",
       "\n",
       "            FacPAT_avg           TxsPAT_avg          FacCCPC_avg  \\\n",
       "0              0.00000              0.00000         15,511.25000   \n",
       "1              0.00000              0.00000         62,203.66667   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3              0.00000              0.00000         20,909.16667   \n",
       "4              0.00000              0.00000        228,901.25000   \n",
       "\n",
       "           TxsCCPC_avg          FacCCOT_avg          TxsCCOT_avg  \\\n",
       "0              0.66667          6,460.75000              0.25000   \n",
       "1              1.58333          8,916.66667              0.33333   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3              1.16667          1,082.50000              0.08333   \n",
       "4              4.33333          3,916.66667              0.08333   \n",
       "\n",
       "            FacCOL_avg           TxsCOL_avg        FacDebCom_avg  \\\n",
       "0              0.00000              0.00000        178,764.16667   \n",
       "1              0.00000              0.00000              0.00000   \n",
       "2              0.00000              0.00000        118,568.66667   \n",
       "3              0.00000              0.00000              0.00000   \n",
       "4              0.00000              0.00000              0.00000   \n",
       "\n",
       "         TxsDebCom_avg        FacDebAtm_avg        TxsDebAtm_avg  \\\n",
       "0             28.91667         99,000.00000              4.25000   \n",
       "1              0.00000              0.00000              0.00000   \n",
       "2              6.75000        741,071.75000             15.00000   \n",
       "3              0.00000              0.00000              0.00000   \n",
       "4              0.00000              0.00000              0.00000   \n",
       "\n",
       "               Col_avg          ColL1T0_avg          ColL1TE_avg  \\\n",
       "0        920,109.33333         15,059.41667        842,900.75000   \n",
       "1        513,504.12917          7,469.83333        309,153.41667   \n",
       "2        117,984.83333        117,984.83333              0.00000   \n",
       "3        340,728.08333         14,826.16667        295,497.25000   \n",
       "4        242,907.25000         51,661.25000             17.66667   \n",
       "\n",
       "           ColL2T0_avg          ColL2AC_avg          ColL2CC_avg  \\\n",
       "0         27,449.50000              0.00000         34,699.66667   \n",
       "1        133,584.41667              0.00000         17,597.25000   \n",
       "2              0.00000              0.00000              0.00000   \n",
       "3         27,698.16667              0.00000          2,706.50000   \n",
       "4        185,000.66667              0.00000          6,227.66667   \n",
       "\n",
       "             ColMx_avg          PagoNac_avg          PagoInt_avg  \\\n",
       "0              0.00000         29,333.33333              0.00000   \n",
       "1         45,699.21250        167,416.66667              0.00000   \n",
       "2              0.00000         68,325.75000              0.00000   \n",
       "3              0.00000         57,083.33333              0.00000   \n",
       "4              0.00000        291,317.16667              0.00000   \n",
       "\n",
       "           EeccNac_avg          EeccInt_avg            UsoL1_avg  \\\n",
       "0        908,079.41667              0.00000        913,045.58333   \n",
       "1        460,136.50000             58.90417        428,734.83333   \n",
       "2          8,510.75000              0.00000         25,294.50000   \n",
       "3        341,916.83333              0.00000        344,417.50000   \n",
       "4        291,684.50000              0.00000         83,849.25000   \n",
       "\n",
       "             UsoL2_avg            UsoLI_avg  \n",
       "0         53,874.25000              0.00000  \n",
       "1        119,963.91667             84.50833  \n",
       "2              0.00000              0.00000  \n",
       "3         31,825.25000              0.00000  \n",
       "4        402,597.66667              0.00000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = monopoly.drop(columns=['Renta'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SeparaciÃ³n de datos de entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size= 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10224, 65)\n",
      "(10224,)\n"
     ]
    }
   ],
   "source": [
    "#Conjunto de entrenamiento\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40897, 65)\n",
      "(40897,)\n"
     ]
    }
   ],
   "source": [
    "#Conjunto de prueba\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from statsmodels.tools.eval_measures import rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2ï¸âƒ£ - `Modelo:` CreaciÃ³n del modelo Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_rf = RandomForestRegressor(n_estimators=100, max_depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ï¸âƒ£ - `Entrenamiento:` Entrenamiento del modelo ajustÃ¡ndolo con Xtrain y Ytrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4ï¸âƒ£ - `PredicciÃ³n:` EstimaciÃ³n de la variable Ytest usando como entrada la variable Xtest. Como resultado obtenemos $\\hat Y$ (Y_hat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimacion de Y (Y \"gorro\")\n",
    "Y_hat = modelo_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5ï¸âƒ£ - `MÃ©tricas:` Evaluamos el desempeÃ±o del modelo a travÃ©s del cÃ¡lculo de mÃ©tricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 175305.05\n",
      "MSE: 93758523678.83\n",
      "RMSE: 306200.14\n",
      "r2: 0.35\n"
     ]
    }
   ],
   "source": [
    "MAE_RF = mean_absolute_error(Y_test, Y_hat)\n",
    "MSE_RF =  mean_squared_error(Y_test, Y_hat)\n",
    "RMSE_RF = math.sqrt(mean_squared_error(Y_test, Y_hat))\n",
    "r2_RF = r2_score(Y_test, Y_hat)\n",
    "\n",
    "print(\"MAE: %.2f\" % MAE_RF)\n",
    "print(\"MSE: %.2f\" % MSE_RF)\n",
    "print(\"RMSE: %.2f\" % RMSE_RF)\n",
    "print('r2: %.2f' % r2_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âž¡ï¸MODEL 2 ðŸŽ„-REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ - `DefiniciÃ³n de variables:` Utilizamos las variables $X$ e $Y$ previamente definidas y divididas en conjunto de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2ï¸âƒ£ - `Modelo:` CreaciÃ³n del modelo Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_rf2 = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=2,min_samples_leaf=2, max_features='sqrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ï¸âƒ£ - `Entrenamiento:` Entrenamiento del modelo ajustÃ¡ndolo con Xtrain y Ytrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, min_samples_leaf=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_rf2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4ï¸âƒ£ - `PredicciÃ³n:` EstimaciÃ³n de la variable Ytest usando como entrada la variable Xtest. Como resultado obtenemos $\\hat Y$ (Y_hat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimacion de Y (Y \"gorro\")\n",
    "Y_hat = modelo_rf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5ï¸âƒ£ - `MÃ©tricas:` Evaluamos el desempeÃ±o del modelo a travÃ©s del cÃ¡lculo de mÃ©tricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE2_M2: 177650.66\n",
      "MSE2_M2: 93960991396.48\n",
      "RMSE_M2: 306530.57\n",
      "r2_M2: 0.34\n"
     ]
    }
   ],
   "source": [
    "MAE_RF = mean_absolute_error(Y_test, Y_hat)\n",
    "MSE_RF =  mean_squared_error(Y_test, Y_hat)\n",
    "RMSE_RF = math.sqrt(mean_squared_error(Y_test, Y_hat))\n",
    "r2_RF = r2_score(Y_test, Y_hat)\n",
    "\n",
    "print(\"MAE2_M2: %.2f\" % MAE_RF)\n",
    "print(\"MSE2_M2: %.2f\" % MSE_RF)\n",
    "print(\"RMSE_M2: %.2f\" % RMSE_RF)\n",
    "print('r2_M2: %.2f' % r2_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âž¡ï¸MODEL 3 ðŸŽ„-REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ - `DefiniciÃ³n de variables:` Utilizamos las variables $X$ e $Y$ previamente definidas y divididas en conjunto de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2ï¸âƒ£ - `Modelo:` CreaciÃ³n del modelo Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\natha\\Desktop\\Nathan_s Folder\\Modelling.ipynb Cell 41\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Nathan_s%20Folder/Modelling.ipynb#Y100sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# GridSearchCV -- mejor combinaciÃ³n de hiperparÃ¡metros\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Nathan_s%20Folder/Modelling.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mmodelo, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneg_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Nathan_s%20Folder/Modelling.ipynb#Y100sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, Y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Nathan_s%20Folder/Modelling.ipynb#Y100sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Obtener la mejor combinaciÃ³n de hiperparÃ¡metros\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/natha/Desktop/Nathan_s%20Folder/Modelling.ipynb#Y100sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m better_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    731\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# HIPERPARAMETROS -- GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Crear una instancia del modelo Random Forest\n",
    "modelo = RandomForestRegressor(random_state=123)\n",
    "\n",
    "# GridSearchCV -- mejor combinaciÃ³n de hiperparÃ¡metros\n",
    "grid_search = GridSearchCV(estimator=modelo, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Obtener la mejor combinaciÃ³n de hiperparÃ¡metros\n",
    "better_params = grid_search.best_params_\n",
    "better_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo Random Forest con los mejores parÃ¡metros\n",
    "modelo_rf3 = RandomForestRegressor(**better_params, random_state=123)\n",
    "\n",
    "'''\n",
    "Se utiliza el doble asterisco (**) para desempaquetar el diccionario de parÃ¡metros\n",
    "(better_params) y pasarlos como argumentos individuales al constructor del modelo.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ï¸âƒ£ - `Entrenamiento:` Entrenamiento del modelo ajustÃ¡ndolo con Xtrain y Ytrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_rf3.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4ï¸âƒ£ - `PredicciÃ³n:` EstimaciÃ³n de la variable Ytest usando como entrada la variable Xtest. Como resultado obtenemos $\\hat Y$ (Y_hat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimacion de Y (Y \"gorro\")\n",
    "Y_hat = modelo_rf3.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5ï¸âƒ£ - `MÃ©tricas:` Evaluamos el desempeÃ±o del modelo a travÃ©s del cÃ¡lculo de mÃ©tricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_RF = mean_absolute_error(Y_test, Y_hat)\n",
    "MSE_RF =  mean_squared_error(Y_test, Y_hat)\n",
    "RMSE_RF = math.sqrt(mean_squared_error(Y_test, Y_hat))\n",
    "r2_RF = r2_score(Y_test, Y_hat)\n",
    "\n",
    "print(\"MAE2_M3: %.2f\" % MAE_RF)\n",
    "print(\"MSE2_M3: %.2f\" % MSE_RF)\n",
    "print(\"RMSE_M3: %.2f\" % RMSE_RF)\n",
    "print('r2_M3: %.2f' % r2_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¶CLASS--MODELS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âž¡ï¸MODEL 1 ðŸŽ„-CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ - `DefiniciÃ³n de variables:` Utilizamos las variables $X$ e $Y$ previamente definidas y divididas en conjunto de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisamos la proporcion en que se encuentra cada clase:\n",
    "print(monopoly.groupby('Region').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = monopoly(['Region'])\n",
    "Y.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = monopoly.drop(columns=['Region', 'Renta'])\n",
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y,test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conjunto de entrenamiento\n",
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)\n",
    "\n",
    "#Conjunto de prueba\n",
    "print(Xtest.shape)\n",
    "print(Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2ï¸âƒ£ - `Modelo:` CreaciÃ³n del modelo Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cf = RandomForestClassifier(n_estimators=50, max_depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ï¸âƒ£ - `Entrenamiento:` Entrenamiento del modelo ajustÃ¡ndolo con Xtrain y Ytrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4ï¸âƒ£ - `PredicciÃ³n:` EstimaciÃ³n de la variable Ytest usando como entrada la variable Xtest. Como resultado obtenemos $\\hat Y$ (Y_hat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5ï¸âƒ£ - `MÃ©tricas:` Evaluamos el desempeÃ±o del modelo a travÃ©s del cÃ¡lculo de mÃ©tricas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
